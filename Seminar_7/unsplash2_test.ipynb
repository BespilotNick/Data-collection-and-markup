{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 YaBrowser/24.1.0.0 Safari/537.36'\n",
    "main_page_url = \"https://unsplash.com\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(f'user-agent={USER_AGENT}')\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "browser = webdriver.Chrome()\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    pages_urls = getting_category_urls(main_page_url)\n",
    "    category_names = getting_category_names(main_page_url)\n",
    "    \n",
    "    unsplash_data_dict = parse_category_page(category_names=category_names, category_urls=pages_urls)\n",
    "\n",
    "\n",
    "    save_data_to_csv(unsplash_data_dict)\n",
    "\n",
    "\n",
    "    browser.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unsplash_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_category_urls(main_page_url: str) -> list:\n",
    "    cat_urls = []\n",
    "    page = BeautifulSoup(requests.get(main_page_url).content, 'html.parser')\n",
    "    categories = page.find(\"div\", {'class': 'pRk2s'}).find_all('ul')\n",
    "    unwanted = page.find(\"div\", {'class': 'pRk2s'}).find('ul').find_all('li', {'class': 'jTN_l'})\n",
    "    for row in categories:\n",
    "        for el in row.find_all('li'):        \n",
    "            if el in unwanted:\n",
    "                continue\n",
    "            else:\n",
    "                cat_urls.append(urljoin(main_page_url, el.find('a').get('href')))\n",
    "    return cat_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_category_names(main_page_url: str) -> list:\n",
    "    cat_names = []\n",
    "    page = BeautifulSoup(requests.get(main_page_url).content, 'html.parser')\n",
    "    categories = page.find(\"div\", {'class': 'pRk2s'}).find_all('ul')\n",
    "    unwanted = page.find(\"div\", {'class': 'pRk2s'}).find('ul').find_all('li', {'class': 'jTN_l'})\n",
    "    for row in categories:\n",
    "        for el in row.find_all('li'):        \n",
    "            if el in unwanted:\n",
    "                continue\n",
    "            else:\n",
    "                cat_names.append(el.find('a').text)\n",
    "    return cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_category_page(category_names: list, category_urls: list) -> dict:\n",
    "    data_dict = {}\n",
    "\n",
    "    for indx, link in enumerate(category_urls):\n",
    "        k = category_names[indx]\n",
    "        data_dict[k] = [parse_photo_page(p_url) for p_url in get_full_page(link)]\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_page(url: str):\n",
    "    try:\n",
    "        browser.get(url)\n",
    "        WebDriverWait(browser, 10).until(ec.presence_of_all_elements_located((By.TAG_NAME, 'body')))\n",
    "        page_height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "        PAUSE_TIME = 2\n",
    "        time.sleep(PAUSE_TIME)\n",
    "\n",
    "        SCROLL_PAUSE_TIME = 1\n",
    "        last_height = page_height\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            new_height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        links = []\n",
    "        pages_paths = '//div[@class=\"ripi6\"]/figure//div[@class=\"zmDAx\"]/a'\n",
    "        pages_links = browser.find_elements(By.XPATH, pages_paths)\n",
    "        for el in pages_links:\n",
    "            links.append(el.get_attribute('href'))\n",
    "\n",
    "    except Exception as E:\n",
    "        print(f'Произошла ошибка, {E}')\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_photo_page(url: str) -> dict:\n",
    "    photo_page = requests.get(url)\n",
    "    page = BeautifulSoup(photo_page.content, 'html.parser')\n",
    "    name_el = page.find('h1')\n",
    "    if name_el is not None:\n",
    "        name = page.find('h1').text\n",
    "    else:\n",
    "        name = \"A Photo Without Name\"\n",
    "    sub_list = []\n",
    "    subcategories = page.find(\"div\", {\"class\": \"MbPKr M5vdR\"}).find_all(\"div\", {\"class\": \"VZRk3 rLPoM\"})\n",
    "    for sub in subcategories:\n",
    "        for el in sub.find_all(\"a\"):\n",
    "            sub_list.append(el.get(\"title\"))\n",
    "    image_url = page.find_all(\"div\", {\"class\": \"MorZF\"})[0].find(\"img\").get(\"src\")\n",
    "\n",
    "    page_dict = {\n",
    "        \"Name\": name,\n",
    "        \"Subcategories\": sub_list,\n",
    "        \"Image_url\": image_url,\n",
    "        \"Local_path\": saving_image(image_url, name)\n",
    "    }\n",
    "\n",
    "    return page_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_image(img_url: str, name: str) -> str:\n",
    "\n",
    "    pic = requests.get(img_url).content\n",
    "    pic_name = f'{name}.jpg'\n",
    "\n",
    "    if not os.path.isdir(\"Unsplash_images\"):\n",
    "        os.mkdir(\"Unsplash_images\")\n",
    "    \n",
    "    os.chdir(\"Unsplash_images\")\n",
    "    \n",
    "    with open(f'{pic_name}', \"wb\") as img:\n",
    "        img.write(pic)\n",
    "\n",
    "    local_path = os.path.abspath(pic_name)\n",
    "\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data: dict) -> None:\n",
    "    with open(\"Unsplash_v2.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Category\", \"Name\", \"Subcategories\", \"Image_url\", \"Local_path\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for key, sublist in data.items():\n",
    "            for i in range(len(sublist)):\n",
    "                row = {}\n",
    "                row[\"Category\"] = key\n",
    "                for row_k, row_v in sublist[i].items():\n",
    "                    row[row_k] = row_v\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
